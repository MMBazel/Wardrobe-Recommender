{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Collection & Initial EDA of Candidate Datasets\n",
    "## Dataset 2: PaperDoll/Chictopia\n",
    "\n",
    "### MLE Capstone: Outfit Recommender - Spring 2021\n",
    "### By: Bazeley, Mikiko \n",
    "### GH: [@mmbazel](https://github.com/MMBazel)  \n",
    "\n",
    "In this notebook, I'll be exploring one of three datasets (DeepFashion, Paperdoll, and iMaterialist). \n",
    "\n",
    "Specifically in this notebook we'll: \n",
    "\n",
    "‚òëÔ∏è Load the data\n",
    "\n",
    "‚òëÔ∏è Explore the dimensions of the dataset\n",
    "\n",
    "‚òëÔ∏è Understand what categories are being represented\n",
    "\n",
    "‚òëÔ∏è Explore samples of the data (the meta data dictionary with catgories & attributes labels, the Train üöÇ file, and finally the images üì∏ themselves)\n",
    "\n",
    "‚òëÔ∏è Understand distributions of categories, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "## Explanation of the data, according to the dataset page here: \n",
    "https://github.com/kyamagu/paperdoll/tree/master/data/chictopia\n",
    "\n",
    "### Files\n",
    "\n",
    "\n",
    "### ‚ö†Ô∏èüìù Notes (About the Notebooks) ‚ö†Ô∏èüìù \n",
    "\n",
    "My guiding principles:\n",
    "* ‚û°Ô∏è Be overly communicative = While that leads to verbose commenting, I hope that means I catch a bunch of questions early)  \n",
    "* ‚û°Ô∏è Human-readable over witty-optimization = For the most part I try to make everything I'm doing obvious\n",
    "* ‚û°Ô∏è Write as much code as needed, and no more = There's a time and place for error-catching & object-oriented code & there are ways to make the notebook reproducible. That's not quite the goal for this notebook (or any of the other notebooks in the early stages of the project) and my goal was to write just the code needed to get this step done.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "# <span style='background :red' > Step 1: Proper set-up & installation of necessary libraries & packages </span> \n",
    "\n",
    "1. Ensure you're using the right flavor of commands and that you have sqlite3 available to you. It's easy enough to pull up the terminal or whatever shell version youre using to check. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=c64acaff6bce3dd1bec05890c69dc1e3522a8e24a9c1dd0ce1927392ca0cda04\n",
      "  Stored in directory: /Users/mikikobazeley/Library/Caches/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install lmdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n",
      "['Image', 'In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i2', '_ih', '_ii', '_iii', '_oh', 'exit', 'get_ipython', 'io', 'json', 'lmdb', 'np', 'os', 'pd', 'plt', 'quit', 'sqlite3', 'sys']\n"
     ]
    }
   ],
   "source": [
    "##################### [TODO] SETUP #####################\n",
    "# [TODO] Import any utilities functions\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import io\n",
    "import lmdb\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Packages Imported')\n",
    "\n",
    "modules = dir()\n",
    "\n",
    "print(modules)\n",
    "#print(os.environ)\n",
    "\n",
    "# [TODO] Package install/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/mikikobazeley/opt/anaconda3/envs/SPRINGBOARD_MLE_CAPSTONE_ENV:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_py-xgboost-mutex         2.0                       cpu_0  \r\n",
      "_pytorch_select           0.1                       cpu_0    anaconda\r\n",
      "_tflow_select             2.3.0                       mkl  \r\n",
      "absl-py                   0.11.0             pyhd3eb1b0_1  \r\n",
      "appnope                   0.1.2           py37hecd8cb5_1001  \r\n",
      "argon2-cffi               20.1.0           py37haf1e3a3_1    anaconda\r\n",
      "astor                     0.8.1            py37hecd8cb5_0  \r\n",
      "async_generator           1.10             py37h28b3542_0    anaconda\r\n",
      "attrs                     20.3.0             pyhd3eb1b0_0  \r\n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \r\n",
      "blas                      1.0                         mkl  \r\n",
      "bleach                    3.3.0              pyhd3eb1b0_0  \r\n",
      "blis                      0.7.4                    pypi_0    pypi\r\n",
      "brotlipy                  0.7.0           py37haf1e3a3_1000    anaconda\r\n",
      "c-ares                    1.17.1               h9ed2024_0  \r\n",
      "ca-certificates           2020.12.5            h033912b_0    conda-forge\r\n",
      "catalogue                 2.0.1                    pypi_0    pypi\r\n",
      "catboost                  0.24.4           py37hf985489_0    conda-forge\r\n",
      "certifi                   2020.12.5        py37hf985489_1    conda-forge\r\n",
      "cffi                      1.14.3           py37hed5b41f_0    anaconda\r\n",
      "chardet                   3.0.4                 py37_1003    conda-forge\r\n",
      "click                     7.1.2              pyh9f0ad1d_0    conda-forge\r\n",
      "colour                    0.1.5                    pypi_0    pypi\r\n",
      "combo                     0.1.2                    pypi_0    pypi\r\n",
      "coverage                  5.4              py37h9ed2024_2  \r\n",
      "cryptography              3.3.1            py37hbcfaee0_0  \r\n",
      "cycler                    0.10.0                   py37_0    anaconda\r\n",
      "cymem                     2.0.5                    pypi_0    pypi\r\n",
      "decorator                 4.4.2              pyhd3eb1b0_0  \r\n",
      "defusedxml                0.6.0                      py_0    conda-forge\r\n",
      "dtreeviz                  1.1.4                    pypi_0    pypi\r\n",
      "entrypoints               0.3                      py37_0    anaconda\r\n",
      "et_xmlfile                1.0.1                   py_1001  \r\n",
      "fastai                    2.2.5                    pypi_0    pypi\r\n",
      "fastcore                  1.3.19                   pypi_0    pypi\r\n",
      "fastprogress              1.0.0                    pypi_0    pypi\r\n",
      "filelock                  3.0.12             pyh9f0ad1d_0    conda-forge\r\n",
      "freetype                  2.10.4               ha233b18_0    conda-forge\r\n",
      "gast                      0.2.2                    py37_0  \r\n",
      "gdown                     3.12.2             pyh9f0ad1d_0    conda-forge\r\n",
      "google-pasta              0.2.0                      py_0  \r\n",
      "grpcio                    1.35.0           py37h97de6d8_0  \r\n",
      "h5py                      2.10.0           py37h0601b69_1  \r\n",
      "hdf5                      1.10.6               hdbbcd12_0  \r\n",
      "idna                      2.10               pyh9f0ad1d_0    conda-forge\r\n",
      "importlib-metadata        3.4.0                    pypi_0    pypi\r\n",
      "importlib_metadata        2.0.0                         1    conda-forge\r\n",
      "iniconfig                 1.1.1                    pypi_0    pypi\r\n",
      "intel-openmp              2019.4                      233    anaconda\r\n",
      "ipykernel                 5.4.3            py37he01cfaa_0    conda-forge\r\n",
      "ipython                   7.20.0           py37h01d92e1_1  \r\n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \r\n",
      "jdcal                     1.4.1                      py_0  \r\n",
      "jedi                      0.17.2           py37hecd8cb5_1  \r\n",
      "jinja2                    2.11.3             pyhd3eb1b0_0  \r\n",
      "joblib                    1.0.0                    pypi_0    pypi\r\n",
      "jpeg                      9b                   he5867d9_2    anaconda\r\n",
      "json5                     0.9.5                      py_0    anaconda\r\n",
      "jsonschema                3.2.0                      py_2    conda-forge\r\n",
      "jupyter_client            6.1.7                      py_0    conda-forge\r\n",
      "jupyter_core              4.7.1            py37hecd8cb5_0  \r\n",
      "jupyterlab                2.2.9              pyhd8ed1ab_0    conda-forge\r\n",
      "jupyterlab_pygments       0.1.2                      py_0    anaconda\r\n",
      "jupyterlab_server         1.2.0                      py_0    conda-forge\r\n",
      "keras-applications        1.0.8                      py_1  \r\n",
      "keras-preprocessing       1.1.2              pyhd3eb1b0_0  \r\n",
      "kiwisolver                1.2.0            py37h04f5b5a_0    anaconda\r\n",
      "lcms2                     2.11                 h92f6f08_0    anaconda\r\n",
      "libcxx                    10.0.0                        1    conda-forge\r\n",
      "libedit                   3.1.20191231         h1de35cc_1    anaconda\r\n",
      "libffi                    3.3                  hb1e8313_2    anaconda\r\n",
      "libgfortran               3.0.1                h93005f0_2  \r\n",
      "libllvm10                 10.0.1               h76017ad_5  \r\n",
      "libmklml                  2019.0.5                      0    anaconda\r\n",
      "libpng                    1.6.37               ha441bb4_0    anaconda\r\n",
      "libprotobuf               3.14.0               h2842e9f_0  \r\n",
      "libsodium                 1.0.18               h1de35cc_0    anaconda\r\n",
      "libtiff                   4.1.0                hcb84e12_1  \r\n",
      "libxgboost                0.90                 hb1e8313_1  \r\n",
      "llvm-openmp               10.0.0               h28b9765_0    conda-forge\r\n",
      "llvmlite                  0.35.0                   pypi_0    pypi\r\n",
      "lmdb                      1.1.1                    pypi_0    pypi\r\n",
      "lz4-c                     1.9.3                h23ab428_0  \r\n",
      "markdown                  3.3.3            py37hecd8cb5_0  \r\n",
      "markupsafe                1.1.1            py37h1de35cc_0    conda-forge\r\n",
      "matplotlib                3.3.2                hecd8cb5_0  \r\n",
      "matplotlib-base           3.3.2            py37h181983e_0  \r\n",
      "mistune                   0.8.4            py37h1de35cc_0    anaconda\r\n",
      "mkl                       2019.4                      233    anaconda\r\n",
      "mkl-service               2.3.0            py37hfbe908c_0    anaconda\r\n",
      "mkl_fft                   1.2.0            py37hc64f4ea_0    anaconda\r\n",
      "mkl_random                1.1.1            py37h959d312_0    anaconda\r\n",
      "murmurhash                1.0.5                    pypi_0    pypi\r\n",
      "nbclient                  0.5.1                      py_0    conda-forge\r\n",
      "nbconvert                 6.0.7                    py37_0    anaconda\r\n",
      "nbformat                  5.1.2              pyhd3eb1b0_1  \r\n",
      "ncurses                   6.2                  h0a44026_1    anaconda\r\n",
      "nest-asyncio              1.4.3              pyhd3eb1b0_0  \r\n",
      "ninja                     1.10.2           py37hf7b0b51_0  \r\n",
      "notebook                  6.2.0            py37hecd8cb5_0  \r\n",
      "numba                     0.52.0                   pypi_0    pypi\r\n",
      "numpy                     1.19.1           py37h3b9f5b6_0    anaconda\r\n",
      "numpy-base                1.19.1           py37hcfb5961_0    anaconda\r\n",
      "olefile                   0.46                       py_0    conda-forge\r\n",
      "openpyxl                  3.0.6              pyhd3eb1b0_0  \r\n",
      "openssl                   1.1.1j               hbcf498f_0    conda-forge\r\n",
      "opt_einsum                3.1.0                      py_0  \r\n",
      "packaging                 20.9               pyhd3eb1b0_0  \r\n",
      "pandas                    1.2.1            py37hb2f4e1b_0  \r\n",
      "pandoc                    2.11                 h0dc7051_0    anaconda\r\n",
      "pandocfilters             1.4.3            py37hecd8cb5_1  \r\n",
      "parso                     0.7.0                      py_0    anaconda\r\n",
      "pathy                     0.3.5                    pypi_0    pypi\r\n",
      "patsy                     0.5.1                    py37_0  \r\n",
      "pdpbox                    0.2.0                    pypi_0    pypi\r\n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \r\n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \r\n",
      "pillow                    8.0.0            py37h1a82f1a_0    anaconda\r\n",
      "pip                       20.2.4                   py37_0    anaconda\r\n",
      "pluggy                    0.13.1                   pypi_0    pypi\r\n",
      "preshed                   3.0.5                    pypi_0    pypi\r\n",
      "prometheus_client         0.9.0              pyhd3eb1b0_0  \r\n",
      "prompt-toolkit            3.0.8                      py_0    conda-forge\r\n",
      "protobuf                  3.14.0           py37h23ab428_1  \r\n",
      "psutil                    5.8.0                    pypi_0    pypi\r\n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \r\n",
      "py                        1.10.0                   pypi_0    pypi\r\n",
      "py-xgboost                0.90             py37hb1e8313_1    anaconda\r\n",
      "pycparser                 2.20               pyh9f0ad1d_2    conda-forge\r\n",
      "pydantic                  1.7.3                    pypi_0    pypi\r\n",
      "pydotplus                 2.0.2                    py37_1  \r\n",
      "pygments                  2.7.4              pyhd3eb1b0_0  \r\n",
      "pyod                      0.8.6              pyh44b312d_0    conda-forge\r\n",
      "pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge\r\n",
      "pyparsing                 2.4.7              pyhd3eb1b0_0  \r\n",
      "pyrsistent                0.17.3           py37haf1e3a3_0    anaconda\r\n",
      "pysocks                   1.7.1                    py37_0    conda-forge\r\n",
      "pytest                    6.2.2                    pypi_0    pypi\r\n",
      "python                    3.7.9                h26836e1_0    anaconda\r\n",
      "python-dateutil           2.8.1              pyhd3eb1b0_0  \r\n",
      "python-dotenv             0.15.0             pyhd8ed1ab_0    conda-forge\r\n",
      "python-graphviz           0.16                     pypi_0    pypi\r\n",
      "python_abi                3.7                     1_cp37m    conda-forge\r\n",
      "pytz                      2021.1             pyhd3eb1b0_0  \r\n",
      "pyyaml                    5.4.1                    pypi_0    pypi\r\n",
      "pyzmq                     20.0.0           py37h23ab428_1  \r\n",
      "readline                  8.1                  h9ed2024_0  \r\n",
      "requests                  2.25.1             pyhd3deb0d_0    conda-forge\r\n",
      "scikit-learn              0.24.1                   pypi_0    pypi\r\n",
      "scipy                     1.6.0            py37h2515648_0  \r\n",
      "seaborn                   0.11.1             pyhd3eb1b0_0  \r\n",
      "send2trash                1.5.0              pyhd3eb1b0_1  \r\n",
      "setuptools                50.3.0           py37h0dc7051_1    anaconda\r\n",
      "six                       1.15.0                     py_0    anaconda\r\n",
      "smart-open                3.0.0                    pypi_0    pypi\r\n",
      "spacy                     3.0.1                    pypi_0    pypi\r\n",
      "spacy-legacy              3.0.1                    pypi_0    pypi\r\n",
      "sqlite                    3.33.0               hffcf06c_0    anaconda\r\n",
      "srsly                     2.4.0                    pypi_0    pypi\r\n",
      "statsmodels               0.12.0           py37haf1e3a3_0  \r\n",
      "suod                      0.0.6                    pypi_0    pypi\r\n",
      "tensorboard               2.0.0              pyhb38c66f_1  \r\n",
      "tensorflow                2.0.0           mkl_py37hda344b4_0  \r\n",
      "tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  \r\n",
      "tensorflow-estimator      2.0.0              pyh2649769_0  \r\n",
      "termcolor                 1.1.0            py37hecd8cb5_1  \r\n",
      "terminado                 0.9.2            py37hecd8cb5_0  \r\n",
      "testpath                  0.4.4              pyhd3eb1b0_0  \r\n",
      "thinc                     8.0.1                    pypi_0    pypi\r\n",
      "threadpoolctl             2.1.0              pyh5ca1d4c_0  \r\n",
      "tk                        8.6.10               hb0a8c7a_0    conda-forge\r\n",
      "toml                      0.10.2                   pypi_0    pypi\r\n",
      "torch                     1.7.1                    pypi_0    pypi\r\n",
      "torchvision               0.8.2                    pypi_0    pypi\r\n",
      "tornado                   6.0.4            py37h1de35cc_1    anaconda\r\n",
      "tqdm                      4.56.0             pyhd8ed1ab_0    conda-forge\r\n",
      "traitlets                 5.0.5              pyhd3eb1b0_0  \r\n",
      "typer                     0.3.2                    pypi_0    pypi\r\n",
      "typing-extensions         3.7.4.3                  pypi_0    pypi\r\n",
      "urllib3                   1.26.3             pyhd8ed1ab_0    conda-forge\r\n",
      "wasabi                    0.8.2                    pypi_0    pypi\r\n",
      "wcwidth                   0.2.5                      py_0    anaconda\r\n",
      "webencodings              0.5.1                    py37_1    anaconda\r\n",
      "werkzeug                  1.0.1              pyhd3eb1b0_0  \r\n",
      "wheel                     0.36.2             pyhd3eb1b0_0  \r\n",
      "wrapt                     1.12.1           py37h1de35cc_1  \r\n",
      "xlrd                      2.0.1              pyhd3eb1b0_0  \r\n",
      "xz                        5.2.5                h1de35cc_0    anaconda\r\n",
      "zeromq                    4.3.3                hb1e8313_3    anaconda\r\n",
      "zipp                      3.4.0              pyhd3eb1b0_0  \r\n",
      "zlib                      1.2.11               h1de35cc_3    anaconda\r\n",
      "zstd                      1.4.5                h41d2c2f_0    anaconda\r\n"
     ]
    }
   ],
   "source": [
    "# Confirm all the right libraries are present\n",
    "# This is an important step because there's a good chance\n",
    "# that for some pckaes where you use pip or pip3 install\n",
    "# they could download to the wrong directory if you're not\n",
    "# using the right pip executable\n",
    "\n",
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikikobazeley/Github/personal/MMBazel/Wardrobe-Recommender/notebooks/Step2_EDA\r\n"
     ]
    }
   ],
   "source": [
    "# Confirm path of working directory\n",
    "!pwd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "# <span style='background :red' > Step 2: Download the Chictopia Datasets</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Best way to not mess this process up is to follow the insteructions here exactly: https://github.com/kyamagu/paperdoll/tree/master/data/chictopia\n",
    "\n",
    "Make sure you have installed: \n",
    "‚û°Ô∏è wget (I used homebrew to install & manage so it's globally available on my mac as opposed to just the conda env => !brew install wget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Clone the Github Repo </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/kyamagu/paperdoll /Volumes/MiniGator/Projects/Datasets/Paperdoll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m.\u001b[m\u001b[m/              \u001b[30m\u001b[43m.git\u001b[m\u001b[m/           \u001b[31m.gitignore\u001b[m\u001b[m*     \u001b[31mREADME.md\u001b[m\u001b[m*\r\n",
      "\u001b[30m\u001b[43m..\u001b[m\u001b[m/             \u001b[31m.gitattributes\u001b[m\u001b[m* \u001b[31mLICENSE.txt\u001b[m\u001b[m*    \u001b[30m\u001b[43mdata\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "# Confirm zipfile has been downloaded to the repo\n",
    "# Replace path with the path-to-dowloaded-dataset\n",
    "!ls -a -F /Volumes/MiniGator/Projects/Datasets/Paperdoll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m.\u001b[m\u001b[m/          \u001b[30m\u001b[43m..\u001b[m\u001b[m/         \u001b[31m.gitignore\u001b[m\u001b[m* \u001b[30m\u001b[43mchictopia\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a -F /Volumes/MiniGator/Projects/Datasets/Paperdoll/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m.\u001b[m\u001b[m/         \u001b[30m\u001b[43m..\u001b[m\u001b[m/        \u001b[31mREADME.md\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a -F /Volumes/MiniGator/Projects/Datasets/Paperdoll/data/chictopia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-ap-northeast-1.amazonaws.com/kyamagu-public/chictopia2/photos.lmdb.tar -P /Volumes/MiniGator/Projects/Datasets/Paperdoll/data/chictopia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13707429/decompress-gzip-file-to-specific-directory\n",
    "!tar xf /Volumes/MiniGator/Projects/Datasets/Paperdoll/data/chictopia/photos.lmdb.tar -C /Volumes/MiniGator/Projects/Datasets/Paperdoll/data/chictopia/photos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunzip -c chictopia.sql.gz | sqlite3 chictopia.sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style='background :pink' > ‚¨ÜÔ∏è LEFT OFF ABOVE ‚¨ÜÔ∏è  </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip to the same directory\n",
    "# ! NOTE: If no path is provided, unzip will just unzip files \n",
    "# to your current-working-directory & NOT the same folder\n",
    "\n",
    "\n",
    "# [UNCOMMENT BELOW] To run the command within the Jupyter notebook\n",
    "# !unzip /Volumes/MiniGator/Projects/Datasets/iMaterialist/imaterialist-fashion-2020-fgvc7.zip -d /Volumes/MiniGator/Projects/Datasets/iMaterialist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files and folders, confirming we have all the data \n",
    "!ls -a -F /Volumes/MiniGator/Projects/Datasets/iMaterialist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "# <span style='background :red' > Step 3: Use CLI to start initial exploration üî¨  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to understand:\n",
    "\n",
    "‚òëÔ∏è The data repository structure\n",
    "\n",
    "‚òëÔ∏è Number of files, types of files\n",
    "\n",
    "‚òëÔ∏è Type of information being captured & how it's represented \n",
    "\n",
    "\n",
    "\n",
    "<span style='background :yellow' > üí° Given the size of the dataset, doing initial inspection with shell is fast & easy.\n",
    "No need to break Jupyter before even getting started.  </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list of images in train folder\n",
    "!ls /Volumes/MiniGator/Projects/Datasets/iMaterialist/train | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of images in train folder\n",
    "!ls -a /Volumes/MiniGator/Projects/Datasets/iMaterialist/train | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample list of images in test folder\n",
    "!ls /Volumes/MiniGator/Projects/Datasets/iMaterialist/test | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of images in test folder\n",
    "!ls -a /Volumes/MiniGator/Projects/Datasets/iMaterialist/test | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 20 rows of the sample submission file included, for use in Kaggle competition\n",
    "! head -n 20 /Volumes/MiniGator/Projects/Datasets/iMaterialist/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 20 rows of train metadata\n",
    "! head -n 20 /Volumes/MiniGator/Projects/Datasets/iMaterialist/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 100 lines of the label_descriptions.json\n",
    "! head -n 100 /Volumes/MiniGator/Projects/Datasets/iMaterialist/label_descriptions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "# <span style='background :red' > Step 4A: üî¨ Deeper EDA & Exploration of Dictionary üìö</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òëÔ∏è Explore samples of the data (the meta data dictionary with catgories & attributes labels, the Train üöÇ file, and finally the images üì∏ themselves)\n",
    "\n",
    "‚òëÔ∏è Understand distributions of categories, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Loading the categories and attributes dictionary from JSON </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file & transposng to get the right shape\n",
    "\n",
    "label_descriptions_df = (pd.read_json('/Volumes/MiniGator/Projects/Datasets/iMaterialist/label_descriptions.json',orient='index').T)\n",
    "\n",
    "label_descriptions_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > High-Level Describing </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_descriptions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_descriptions_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Examining individual cells & entries </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corresponds to the first row & first column (categories)\n",
    "label_descriptions_df.loc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corresponds to the first row & second column (attributes)\n",
    "label_descriptions_df.loc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corresponds to the second row & first column (categories)\n",
    "label_descriptions_df.loc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corresponds to the second row & second column (attributes)\n",
    "label_descriptions_df.loc[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This corresponds to the 16th row & first column (categories)\n",
    "label_descriptions_df.loc[15][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Examining Nulls </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The creators of the dataset did something interesting -- they stuck two separate dictioanies together in a single JSON file. So while it looks like there are nulls in the categories column past 46 corresponding to attributes, that's not quite accurate. \n",
    "\n",
    "ü§î As we need to explore the \"nulls\" we need to remember they're not \"true nulls\" but the result of the dataset author's decision to save space & memory by dumping two lists of separate lengths together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's locate all rows of categories that don't have attributes\n",
    "\n",
    "label_desc_isNull_df = label_descriptions_df.loc[pd.isnull(label_descriptions_df['categories'])]\n",
    "label_desc_isNull_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check out some specfic exaples\n",
    "label_desc_isNull_df.loc[46][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.loc[62][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.loc[65][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.loc[293][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.loc[274][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_desc_isNull_df.loc[287][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Cleaning Up Categories & Attributes Dicts </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Cleaning Up Categories </span>\n",
    "\n",
    "We're going to pull out and split the categories list from the attributes list. We'll also make the columns easier to track by renaming, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save off a copy of the dataframe for inspection. \n",
    "# It'll be deleted manually later once we're happy with the quality.\n",
    "\n",
    "categories_notNull_df = label_descriptions_df.loc[pd.notnull(label_descriptions_df['categories'])]\n",
    "categories_notNull_df = pd.json_normalize(categories_notNull_df['categories'])\n",
    "categories_notNull_df.to_csv('../../data/interim/categories_notNull_df.csv')\n",
    "categories_notNull_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_notNull_df = categories_notNull_df.rename(\n",
    "                                columns={'id':'id_categories',\n",
    "                                        'name':'name_categories',\n",
    "                                        'supercategory':'supercategory_categories',\n",
    "                                        'level':'level_categories'})\n",
    "categories_notNull_df = categories_notNull_df.set_index('id_categories')\n",
    "categories_notNull_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking out the distribution of the categories \n",
    "categories_notNull_df['supercategory_categories'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Cleaning Up Attributes </span>\n",
    "\n",
    "Same exact process as we did with categories above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save off a copy of the dataframe for inspection. \n",
    "# It'll be deleted manually later once we're happy with the quality.\n",
    "\n",
    "attributes_df = pd.json_normalize(label_descriptions_df['attributes'])\n",
    "attributes_df.to_csv('../../data/interim/attributes_df.csv')\n",
    "attributes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especially for attributes, it's important we set the id_attributes column as the index\n",
    "# as we intend to use the dataframes for merging later.\n",
    "# The reason why we'd want to set the index instead of just using the default pandas index?\n",
    "# You'll see later but essentially the dataset creators, when creating the JSON file,\n",
    "# skipped numbering the attribute ID's i.e. starting from Attribute ID=281 skip\n",
    "# ahead by 40+. \n",
    "# Lucikly this seems to align with the attribute ID's captured in the Train file.\n",
    "# Keep going, more explanation below.\n",
    "\n",
    "\n",
    "attributes_df = attributes_df.rename(columns={'id':'id_attributes',\n",
    "                                        'name':'name_attributes',\n",
    "                                        'supercategory':'supercategory_attributes',\n",
    "                                        'level':'level_attributes'})\n",
    "attributes_df = attributes_df.set_index('id_attributes')\n",
    "attributes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_df['supercategory_attributes'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid black; margin-top: 1px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "# <span style='background :red' > Step 4B: üî¨ Deeper EDA & Exploration of Train üöÇ File </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Loading the train.csv file üöÇ </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note the following:\n",
    "1. The same image is repeated multiple times but with a unique:\n",
    "    * ClassId\n",
    "    * Encoded Pixels\n",
    "    * AttributesIds\n",
    "    \n",
    "2. Not all ImageIds have corresponding AttributesIds\n",
    "3. AtributeIds are off, starting at AttributeId = 281, which jumps from the prior entry of AttributeId = 234. We also see that the column AttributeIds in the train data_df include AttrbiuteId values of 300+. As part of futher processing we'd need to visually confirm that the train images with AttributeId's of 235-293 are correctly labeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load one chunk to visually examine (so that we dont overload the notebook memory)\n",
    "with pd.read_csv(\"/Volumes/MiniGator/Projects/Datasets/iMaterialist/train.csv\",chunksize=100) as reader:\n",
    "    print(reader.get_chunk(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'll load the file straight; for bigger files howver, it can make sense\n",
    "# to still do lazy loading\n",
    "\n",
    "train_df = pd.read_csv(\"/Volumes/MiniGator/Projects/Datasets/iMaterialist/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Examine the Train file üöÇ</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Get Top-Level Counts of Combined Train üöä  File & Classes Dict üìö </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Train data:\n",
    "# ClassId corresponds to the category ID\n",
    "# Each row in train contains a category label & multiple attribute labels\n",
    "\n",
    "train_df[['ClassId','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['ClassId']).count().merge(categories_notNull_df,how='left',left_on='ClassId',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Visualize entire distribution of supercategory categories </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['ClassId','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['ClassId']).count().merge(categories_notNull_df,how='left',left_on='ClassId',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('supercategory_categories').plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Visualize top 15 categories by supercategory count</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['ClassId','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['ClassId']).count().merge(categories_notNull_df,how='left',left_on='ClassId',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('supercategory_categories').iloc[:15].plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Visualize entire distribution of fine category (not super category) count </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['ClassId','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['ClassId']).count().merge(categories_notNull_df,how='left',left_on='ClassId',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_categories').plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Visualize top 15 of fine category (not super category) count </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['ClassId','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['ClassId']).count().merge(categories_notNull_df,how='left',left_on='ClassId',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_categories').iloc[:15].plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Get Top-Level Counts of Combined Train üöä  File & Attributes Dict üìö </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Select out columns to start analysis of attributes</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: The same image is represented multiple times with a unique ClassId \n",
    "# (aka one image can have multiple items of clothing) and each ClassId (item of clothing)\n",
    "# can have multiple attributes (details like tpye of fabric, buttons, etc)\n",
    "# This is why the AttributesIds column has a list of values\n",
    "\n",
    "train_with_attributes = train_df[['ImageId','ClassId','AttributesIds']]\n",
    "train_with_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background :yellow' > Explode out attributes to make a long train_attributes dataframe </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all images have associated detailed attributes \n",
    "# If we just try to explode out the NaN values, we'll get an error\n",
    "# So we need to convert the AttributesIds into a list of values (str)\n",
    "# such that we can then encapsulate as a list, use pd.explode, \n",
    "# and then recast as int so we can merge on the indices.\n",
    "# Phew!\n",
    "\n",
    "train_with_attributes['AttributesIds'] = train_with_attributes['AttributesIds'].replace(np.nan,-1000).astype(str)\n",
    "train_with_attributes['AttributesIds'] = train_with_attributes['AttributesIds'].apply(lambda x: list(x.split(\",\")))\n",
    "\n",
    "train_with_attributes_long = train_with_attributes.explode('AttributesIds')\n",
    "train_with_attributes_long['AttributesIds'] = train_with_attributes_long['AttributesIds'].astype(int)\n",
    "train_with_attributes_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a count of number of times (not images but occurrences) of the attributes\n",
    "train_with_attributes_long[['AttributesIds','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['AttributesIds']).count().merge(attributes_df,how='left',left_on='AttributesIds',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be an ugly mess of a chart -- long tail but we also have a bunch of the \n",
    "# top-level categories being repeated\n",
    "train_with_attributes_long[['AttributesIds','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['AttributesIds']).count().merge(attributes_df,how='left',left_on='AttributesIds',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_attributes').plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing just the first 15, we see there are a bunch of occurrences of NaN\n",
    "# This isnt surprising as the dataset creators described how only a subset of the original data\n",
    "# had additional detailed attributes information.\n",
    "train_with_attributes_long[['AttributesIds','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['AttributesIds']).count().merge(attributes_df,how='left',left_on='AttributesIds',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_attributes').iloc[:15].plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring Nan, checking out the top 30 attributes\n",
    "train_with_attributes_long[['AttributesIds','ImageId']].rename(columns={'ImageId':'counts_'}).groupby(['AttributesIds']).count().merge(attributes_df,how='left',left_on='AttributesIds',right_index=True).reset_index().sort_values('counts_',ascending=False).set_index('name_attributes').iloc[1:30].plot(kind='barh',y='counts_',use_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background :orange' > Finishing EDA üî¨ by Previewing Some Random Images üì∏ </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Ideally, it would be best to do a random selection of the images to preview by grabbing a list of the image_file names, creating a list of random numbers, then picking the image whose index corresponds to the random number element. \n",
    "\n",
    "ü§î Given that this is just meant to be a cursory preview, we'll do that in later noteboks for the dataset that ultimately gets selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = '/Volumes/MiniGator/Projects/Datasets/iMaterialist/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfImageNames = ['00000663ed1ff0c4e0132b9b9ac53f6e.jpg',\n",
    "                    '0000fe7c9191fba733c8a69cfaf962b7.jpg',\n",
    "                    '0002ec21ddb8477e98b2cbb87ea2e269.jpg',\n",
    "                    '0002f5a0ebc162ecfb73e2c91e3b8f62.jpg',\n",
    "                    '0004467156e47b0eb6de4aa6479cbd15.jpg',\n",
    "                    '00048c3a2fb9c29340473c4cfc06424a.jpg',\n",
    "                    '0006ea84499fd9a06fefbdf47a5eb4c0.jpg'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imageName in listOfImageNames:\n",
    "    display(Image(filename=f'{path_to_dir}//{imageName}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
